{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "estudiantes_q_Learning01_Ejemplo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnahisVelasquez1999/TareaSemana3AV/blob/master/estudiantes_q_Learning01_Ejemplo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybe8CY07zp7D"
      },
      "source": [
        "## Escenario - Robots en un almacén\n",
        "#### Importar Librarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfdhGGMsw1H7"
      },
      "source": [
        "#import libraries\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq-QPfDnx4Fo"
      },
      "source": [
        "## Definir el entorno\n",
        "\n",
        "#### Estados\n",
        "\n",
        "\n",
        "¡Los cuadrados negros y verdes son **estados terminales**!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AdpFVfy6ya9"
      },
      "source": [
        "#define the shape of the environment (i.e., its states)\n",
        "environment_rows = 11\n",
        "environment_columns = 11\n",
        "\n",
        "#Create a 3D numpy array to hold the current Q-values for each state and action pair: Q(s, a) \n",
        "#The array contains 11 rows and 11 columns (to match the shape of the environment), as well as a third \"action\" dimension.\n",
        "#The \"action\" dimension consists of 4 layers that will allow us to keep track of the Q-values for each possible action in\n",
        "#each state (see next cell for a description of possible actions). \n",
        "#The value of each (state, action) pair is initialized to 0.\n",
        "q_values = np.zeros((environment_rows, environment_columns, 4))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07gGSNz07xtP"
      },
      "source": [
        "#### Acciones\n",
        "Las acciones que están disponibles para el agente de IA son mover el robot en una de cuatro direcciones:\n",
        "* Arriba\n",
        "* Derecha\n",
        "* Abajo\n",
        "* Izquierda\n",
        "\n",
        "¡Obviamente, el agente de IA debe aprender a evitar conducir a las ubicaciones de almacenamiento de artículos (por ejemplo, estantes)!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z43QX_t080q3"
      },
      "source": [
        "#define actions\n",
        "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
        "actions = ['up', 'right', 'down', 'left']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X25vn4VKw2as"
      },
      "source": [
        "#### Recompensas\n",
        "El último componente del entorno que necesitamos definir son las **recompensas**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIJu7XsLXw62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07bead55-c45b-48ba-9aed-f6cd04b42cfe"
      },
      "source": [
        "rewards = np.full((environment_rows, environment_columns), -100.)\n",
        "rewards[0, 5] = 100. #set the reward for the packaging area (i.e., the goal) to 100\n",
        "\n",
        "#define aisle locations (i.e., white squares) for rows 1 through 9\n",
        "aisles = {} #store locations in a dictionary\n",
        "aisles[1] = [i for i in range(1, 10)]\n",
        "aisles[2] = [1, 7, 9]\n",
        "aisles[3] = [i for i in range(1, 8)]\n",
        "aisles[3].append(9)\n",
        "aisles[4] = [3, 7]\n",
        "aisles[5] = [i for i in range(11)]\n",
        "aisles[6] = [5]\n",
        "aisles[7] = [i for i in range(1, 10)]\n",
        "aisles[8] = [3, 7]\n",
        "aisles[9] = [i for i in range(11)]\n",
        "\n",
        "#set the rewards for all aisle locations (i.e., white squares)\n",
        "for row_index in range(1, 10):\n",
        "  for column_index in aisles[row_index]:\n",
        "    rewards[row_index, column_index] = -1.\n",
        "  \n",
        "#print rewards matrix\n",
        "for row in rewards:\n",
        "  print(row)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-100. -100. -100. -100. -100.  100. -100. -100. -100. -100. -100.]\n",
            "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.]\n",
            "[-100.   -1. -100. -100. -100. -100. -100.   -1. -100.   -1. -100.]\n",
            "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.   -1. -100.]\n",
            "[-100. -100. -100.   -1. -100. -100. -100.   -1. -100. -100. -100.]\n",
            "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
            "[-100. -100. -100. -100. -100.   -1. -100. -100. -100. -100. -100.]\n",
            "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.]\n",
            "[-100. -100. -100.   -1. -100. -100. -100.   -1. -100. -100. -100.]\n",
            "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
            "[-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFEor01iCCin"
      },
      "source": [
        "## Entrena al modelo\n",
        "\n",
        "#### Definir funciones auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnCfO5tVG0LJ"
      },
      "source": [
        "#define a function that determines if the specified location is a terminal state\n",
        "def is_terminal_state(current_row_index, current_column_index):\n",
        "  #if the reward for this location is -1, then it is not a terminal state (i.e., it is a 'white square')\n",
        "  if rewards[current_row_index, current_column_index] == -1.:\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "#define a function that will choose a random, non-terminal starting location\n",
        "def get_starting_location():\n",
        "\n",
        "  current_row_index = np.random.randint(environment_rows)\n",
        "  current_column_index = np.random.randint(environment_columns)\n",
        "\n",
        "  while is_terminal_state(current_row_index, current_column_index):\n",
        "    current_row_index = np.random.randint(environment_rows)\n",
        "    current_column_index = np.random.randint(environment_columns)\n",
        "    return current_row_index, current_column_index\n",
        "\n",
        "#define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
        "def get_next_action(current_row_index, current_column_index, epsilon):\n",
        "\n",
        "  if np.random.random() < epsilon:\n",
        "    return np.argmax(q_values[current_row_index, current_column_index])\n",
        "  else: \n",
        "    return np.random.randint(4)\n",
        "\n",
        "#define a function that will get the next location based on the chosen action\n",
        "def get_next_location(current_row_index, current_column_index, action_index):\n",
        "  new_row_index = current_row_index\n",
        "  new_column_index = current_column_index\n",
        "  if actions[action_index] == 'up' and current_row_index >0:\n",
        "    new_row_index -=1\n",
        "  elif actions[action_index] == 'right' and current_column_index < environment_columns -1:\n",
        "    new_column_index +=1\n",
        "  elif actions[action_index] == 'down' and current_row_index < environment_rows -1:\n",
        "    new_row_index +=1\n",
        "  elif actions[action_index] == 'left' and current_column_index > 0 :\n",
        "    new_column_index +=1\n",
        "    return new_row_index, new_column_index\n",
        "    \n",
        "def get_shortest_path(start_row_index, start_column_index):\n",
        "      if is_terminal_state(start_row_index, start_column_index):\n",
        "        return []\n",
        "      else:\n",
        "        current_row_index, current_column_index = start_row_index, start_column_index\n",
        "        shortest_path = []\n",
        "        shortest_path.append([current_row_index, current_column_index])\n",
        "\n",
        "      while not is_terminal_state(current_row_index, current_column_index):\n",
        "      \n",
        "        action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
        "        \n",
        "        current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
        "        shortest_path.append([current_row_index, current_column_index])\n",
        "        return shortest_path"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjl9niKEqONs"
      },
      "source": [
        "#### Entrenar el Agente de IA usando Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N5BB0m0JHIn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "36e0a437-a295-4841-c633-5c898036292f"
      },
      "source": [
        "#define training parameters\n",
        "epsilon = 0.9 #the percentage of time when we should take the best action (instead of a random action)\n",
        "discount_factor = 0.9 #discount factor for future rewards\n",
        "learning_rate = 0.9 #the rate at which the AI agent should learn\n",
        "\n",
        "#run through 1000 training episodes\n",
        "for episode in range(1000):\n",
        "  #get the starting location for this episode\n",
        "  row_index, column_index = get_starting_location()\n",
        "\n",
        "  #continue taking actions (i.e., moving) until we reach a terminal state\n",
        "  #(i.e., until we reach the item packaging area or crash into an item storage location)\n",
        "  while not is_terminal_state(row_index, column_index):\n",
        "    #choose which action to take (i.e., where to move next)\n",
        "    action_index = get_next_action(row_index, column_index, epsilon)\n",
        "\n",
        "    #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
        "    old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
        "    row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
        "    \n",
        "    #receive the reward for moving to the new state, and calculate the temporal difference\n",
        "    reward = rewards[row_index, column_index]\n",
        "    old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
        "    temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index]))\n",
        "\n",
        "    #update the Q-value for the previous state and action pair\n",
        "    new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
        "    q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
        "\n",
        "print('Training complete!')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b7368e63f143>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m#get the starting location for this episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mrow_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_starting_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m#continue taking actions (i.e., moving) until we reach a terminal state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JqQfjYdfyBh"
      },
      "source": [
        "## Obtenga las rutas más cortas\n",
        "Ahora que el agente de IA ha sido completamente capacitado, podemos ver lo que ha aprendido al mostrar la ruta más corta entre cualquier ubicación en el almacén donde el robot puede viajar y el área de empaque del artículo.\n",
        "****\n",
        "¡Ejecute la celda de código a continuación para probar algunas ubicaciones de inicio diferentes!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1YO3mj_oS2J"
      },
      "source": [
        "#display a few shortest paths\n",
        "print(get_shortest_path(3, 9)) #starting at row 3, column 9\n",
        "print(get_shortest_path(5, 0)) #starting at row 5, column 0\n",
        "print(get_shortest_path(9, 5)) #starting at row 9, column 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWx7BsJxqrDv"
      },
      "source": [
        "#### Por fin...\n",
        "Es genial que nuestro robot pueda tomar automáticamente el camino más corto desde cualquier ubicación 'legal' en el almacén hasta el área de empaque del artículo. **Pero, ¿qué pasa con el escenario opuesto?**\n",
        "\n",
        "Dicho de otra manera, nuestro robot actualmente puede entregar un artículo desde cualquier lugar del almacén ***a*** el área de embalaje, pero después de entregar el artículo, deberá viajar ***desde*** el área de embalaje a otra ubicación en el almacén para recoger el siguiente artículo!\n",
        "\n",
        "No se preocupe, este problema se resuelve fácilmente ***invirtiendo el orden de la ruta más corta***.\n",
        "\n",
        "Ejecute la siguiente celda de código para ver un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKun8LInsas9"
      },
      "source": [
        "#display an example of reversed shortest path\n",
        "path = get_shortest_path\n",
        "print(path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}